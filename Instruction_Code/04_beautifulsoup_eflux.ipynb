{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y9alpuLKqw5"
      },
      "source": [
        "# Web Scraping with Beautiful Soup\n",
        "*[Beautiful Soup](https://pypi.org/project/beautifulsoup4/)*  is a Python library for pulling data out of HTML and XML files. It can be used to scrape data, text, links, or image urls from within a website.\n",
        "<br>\n",
        "In addition to going step-by-step through scraping web text with BeautifulSoup, we will cover these core Python topics:\n",
        "<br>\n",
        "    Regular Expressions\n",
        "    <br>\n",
        "    List Comprehension\n",
        "    <br>\n",
        "    For loops\n",
        "    <br>\n",
        "    Output data to file (.txt)\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB_CINGaKqw-"
      },
      "source": [
        "#pip install beautifulsoup4\n",
        "#above line not required in Colab, as bs4 is preinstalled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaY0XQXcKqw_"
      },
      "source": [
        "After installing, import `BeautifulSoup`, as well as `requests` for loading websites and `re` for using regular expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLTuPnGHKqxA"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests, re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36AHkIZlKqxA"
      },
      "source": [
        "Here, we use `requests.get` to load our website. In this example we are interested in downloading articles from eFlux Architecture's project *Superhumanity.*\n",
        "<br>\n",
        "Next, we use `BeautifulSoup` to grab and print the html content of the site. This will be messy at first, but we will learn to look through the html for the desired links."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpBnt1NAKqxB"
      },
      "source": [
        "url = 'https://www.e-flux.com/architecture/superhumanity/'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "print(soup.prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of-e3BeRKqxB"
      },
      "source": [
        "## Scraping html for links\n",
        "One common task is extracting all the URLs found within a pageâ€™s < a > tags. The link itself is often labeled href as can be seen in the following code:\n",
        "    \n",
        "`<a class=\"preview-item-details-title js-open-overlay is-architecture\" data-pagetitle=\"Our Heads Are Round, Our Hands Irregular - Superhumanity - e-flux\" data-topline=\"Superhumanity - Hu Fang - Our Heads Are Round, Our Hands Irregular\" href=\"/architecture/superhumanity/68654/our-heads-are-round-our-hands-irregular/\">`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiB3aePyKqxC"
      },
      "source": [
        "links = soup.find_all(\"a\", {\"href\": True})\n",
        "print(links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3xUQQGHKqxD"
      },
      "source": [
        "While the output above still contains all of the < a > tag data (titles, authours, as well as links), the following will print out ONLY the links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Vh0RsBKqxE"
      },
      "source": [
        "for o in links:\n",
        "    print(o.attrs[\"href\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsiNVM_2KqxF"
      },
      "source": [
        "Rather than just printing the links, let's `append` them to a new list that only contains the links.\n",
        "\n",
        "<br>\n",
        "The for loop in this example simply goes through our big, messy list of links, authors, titles, etc... and adds *just* the links to a new list\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLoP7qnTKqxF"
      },
      "source": [
        "# make an empty list for our links\n",
        "link_list = []\n",
        "for o in links:\n",
        "    link_list.append(o.attrs[\"href\"])\n",
        "print(link_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svu4LTGoKqxF"
      },
      "source": [
        "At this point, we have a list containing all of our links, but it also contains some unwanted links (to eFlux social media, parent links, etc.)\n",
        "\n",
        "The next cell will run a trick called [list comprehension](https://docs.python.org/3/tutorial/datastructures.html) to only keep those list elements that contain this string `\"/architecture/superhumanity/\"`\n",
        "\n",
        "A list comprehension is a way of simplifying a process that may otherwise be done in a for loop, so it is a tool built in to Python for modifying or making a new list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTzoIQd8KqxG"
      },
      "source": [
        "new_list = [x for x in link_list if \"/architecture/superhumanity/\" in x]\n",
        "new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One problem we see with the above list is duplicate entries, we will address that later"
      ],
      "metadata": {
        "id": "YmF7RDs3G20R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw9kOYTJKqxG"
      },
      "source": [
        "## Scraping html for bodies of text\n",
        "Great! Now that we have our list of links, we want to follow each one and scrape the textual content from each of the pages.\n",
        "The following code is identical to cell #3 where we first used `requests`... except this time we are looking at one of the article pages, not the higher-level \"superhumanity\" page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buIkZhp3KqxH"
      },
      "source": [
        "p = requests.get(\"https://www.e-flux.com/architecture/superhumanity/91055/art-without-death/\") \n",
        "mysoup = BeautifulSoup(p.content)\n",
        "\n",
        "print(mysoup.prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yhpXrFdKqxH"
      },
      "source": [
        "Once again we see a big mess of html, but it is fairly clear that the article text begins with the tag `<div class=\"article__body\">`\n",
        "So we will once again use `.find_all` to isolate the block-text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtzYG6JzKqxI"
      },
      "source": [
        "text = mysoup.find_all(\"div\", {\"class\":\"article__body\"})\n",
        "text_string = str(text)\n",
        "print(text_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCYsvO1UKqxI"
      },
      "source": [
        "There are just a few formatting issues to take care of using `re`... regular expressions. First we delete anything between < carrots >.\n",
        "\n",
        "<br>\n",
        "Before proceeding, let's look at a specific issue with regular expressions. In our previous example with wikipedia, we could remove everything between the ==headers== with the following regex: ==.*==+\n",
        "\n",
        "<br>\n",
        "So we may assume we can remove everything between the carrots with something like <.*>... but the carrots have their own specific meaning in regex, so we need to separate them from other metacharacters. \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "`re.sub(r'` defines this as a substitution with regular expressions\n",
        "<br>\n",
        "`<` starting with left carrot\n",
        "<br>\n",
        "`[^>]+` any characters that are not a right carrot\n",
        "<br>\n",
        "`>` ending with a right carrot\n",
        "<br>\n",
        "....replace all of this with `''` an empty string\n",
        "<br>\n",
        "\n",
        "then replace `\\n` new lines with spaces `text_string.replace('\\n', ' ')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaHmpa76KqxJ"
      },
      "source": [
        "text_string = re.sub(r'<[^>]+>', '', text_string)\n",
        "text_string = text_string.replace('\\n', ' ')\n",
        "(text_string)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urkJb0enKqxJ"
      },
      "source": [
        "So, here we have the entire article text from that single eFlux plage. But remember, we have a whole list of 125 other pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a-X6XzRKqxJ"
      },
      "source": [
        "print(len(new_list))\n",
        "new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dai9RCFsPQxc"
      },
      "source": [
        "It looks like there are some duplicate links in the above list. Here we can use another list comprehension to eliminate any repeated elements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn2K8ZCPKqxJ"
      },
      "source": [
        "scrubbed_list = []\n",
        "[scrubbed_list.append(x) for x in new_list if x not in scrubbed_list]\n",
        "scrubbed_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LneslvSKqxK"
      },
      "source": [
        "The following loop brings everthing together, downloading each link, scraping the body text, scrubbing with regex and combining into one long string (this will take a couple minutes to process all 125 articles)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0D1bbU0KqxK"
      },
      "source": [
        "collect = ''\n",
        "for link in scrubbed_list:\n",
        "#make whole url from e-flux + list element\n",
        "    url = (\"https://www.e-flux.com\" + link) \n",
        "    print(url)\n",
        "#get html\n",
        "    s = requests.get(url) \n",
        "    soup = BeautifulSoup(s.content)\n",
        "#grab the block-text\n",
        "    text = soup.find_all(\"div\", {\"class\":\"article__body\"})\n",
        "    text_str = str(text)\n",
        "#regex    \n",
        "    text_str = re.sub(r'<[^>]+>', '', text_str)\n",
        "    text_str = text_str.replace('\\n', ' ')\n",
        "#collect all texts\n",
        "    collect = collect + text_str\n",
        "print(collect)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rcnLYKyKqxK"
      },
      "source": [
        "Let's check the word count of that long string of combined articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkiPSj81KqxK"
      },
      "source": [
        "print(str(len(collect.split())) + \" appx words\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8wbgPbYKqxL"
      },
      "source": [
        "And finally, we can write the complete collected text to a singe .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI_jICYGKqxL"
      },
      "source": [
        "text_file = open(\"eflux.txt\", \"w\")\n",
        "n = text_file.write(collect)\n",
        "text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a2xvfwPZbRZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}