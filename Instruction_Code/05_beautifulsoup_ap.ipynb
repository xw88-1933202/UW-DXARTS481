{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-kYMHYOWYRb"
      },
      "source": [
        "# Web Scraping with Beautiful Soup\n",
        "*Beautiful Soup*  is a Python library for pulling data out of HTML and XML files. It can be used to scrape data, text, links, or image urls from within a website.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6os9r7WWYRd"
      },
      "source": [
        "#pip install beautifulsoup4\n",
        "#above line not required in Colab, as bs4 is preinstalled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz39cqJmWYRe"
      },
      "source": [
        "After installing, import `BeautifulSoup`, as well as `requests` for loading websites and `re` for using regular expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-p4jwf1WYRe"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests, re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXvSNXAXWYRe"
      },
      "source": [
        "Here, we use `requests.get` to load our website. In this example we are interested in downloading articles from the Associated Press *Politics* page.\n",
        "<br>\n",
        "Next, we use `BeautifulSoup` to grab and print the html content of the site. This will be messy at first, but we will learn to look through the html for the desired links."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph1D1QVYWYRf"
      },
      "source": [
        "s = requests.get(\"https://apnews.com/hub/politics\") \n",
        "soup = BeautifulSoup(s.content)\n",
        "\n",
        "print(soup.prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7cXdreUWYRf"
      },
      "source": [
        "## Scraping html for links\n",
        "One common task is extracting all the URLs found within a pageâ€™s < a > tags. The link itself is often labeled href as can be seen in the following code:\n",
        "    \n",
        "`<a class=\"Component-headline-0-2-110\" data-key=\"card-headline\" href=\"/article/election-2020-virus-outbreak-joe-biden-campaigns-kamala-harris-aa0bb12aca5568d20a7d6b86f24da7d0\">`\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGm_IZkjWYRf"
      },
      "source": [
        "links = soup.find_all(\"a\", {\"href\": True})\n",
        "print(links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehH_8-XeWYRg"
      },
      "source": [
        "While the output above still contains all of the < a > tag data (titles, authours, as well as links), the following will sort out ONLY the links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOnBBfehWYRg"
      },
      "source": [
        "for o in links:\n",
        "    print(o.attrs[\"href\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT9dSIcOWYRg"
      },
      "source": [
        "Rather than just printing the links, let's `append` them to a new list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW560xmlWYRg"
      },
      "source": [
        "link_list =[]\n",
        "for o in links:\n",
        "    link_list.append(o.attrs[\"href\"])\n",
        "print(link_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnrsnPnOWYRh"
      },
      "source": [
        "At this point, we have a list containing all of our links, but it also contains some unwanted links (to AP social media, etc.)\n",
        "\n",
        "The next cell will run a trick called [list comprehension](https://docs.python.org/3/tutorial/datastructures.html) to only keep those list elements that contain the string `\"/article\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9kF9rM-WYRh"
      },
      "source": [
        "new_list=[x for x in link_list if \"/article\" in x]\n",
        "new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwCKEbalWYRh"
      },
      "source": [
        "Finally, we see many duplicate/triplicate links, but we can use a quick trick to eliminate dupes. `Lists` can contain duplicates, but the `dictionary` data type cannot. So the following cell simply turns `new_list` into a dictionary then back into a list, thus eliminating dupes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU-SShOWWYRh"
      },
      "source": [
        "new_list = list(dict.fromkeys(new_list))\n",
        "new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNGA0W1WYRi"
      },
      "source": [
        "## Scraping html for bodies of text\n",
        "Great! Now that we have our list of links, we want to follow each one and scrape the textual content from each of the pages.\n",
        "The following code is identical to cell #3 where we first used `requests`\n",
        "Note: url is created by adding apnews.com to the first element of our `new_list` of links \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z30oFB1eWYRi"
      },
      "source": [
        "url = (\"https://apnews.com\" + new_list[1])\n",
        "print(url)\n",
        "p = requests.get(url) \n",
        "mysoup = BeautifulSoup(p.content)\n",
        "print(mysoup.prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKCeFW1HWYRi"
      },
      "source": [
        "Once again we see a big mess of html, but it is fairly clear that the article text begins with the tag `<div class=\"Article\" data-key=\"article\">`\n",
        "So we will once again use `.find_all` to isolate the article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfi4sQW0WYRj"
      },
      "source": [
        "text = mysoup.find_all(\"div\", {\"class\": \"Article\"})\n",
        "text_string = str(text)\n",
        "print(text_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxCufYQPWYRj"
      },
      "source": [
        "There are just a few formatting issues to take care of using `re`... regular expressions. First we delete anything between < carrots >... ~then replace `\\n` new lines with spaces~ (AP doesn't appear to have newline characters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFdx9Gm5WYRj"
      },
      "source": [
        "text_string = re.sub(r'<[^>]+>', ' ', text_string)\n",
        "#text_string = text_string.replace('\\n', ' ')\n",
        "#print(text_string)\n",
        "text_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35lKhJ51WYRk"
      },
      "source": [
        "So, here we have the entire article text from that single AP article. But remember, we have a whole list of 50+ articles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR0Wv5sAWYRk"
      },
      "source": [
        "print(len(new_list))\n",
        "new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aig9gCSEZXuU"
      },
      "source": [
        "For the sake of demonstration, lets shorten this to 10:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0wymz4DZXZb"
      },
      "source": [
        "del(new_list[10:])\n",
        "print(len(new_list))\n",
        "print(new_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXM_fQ_wWYRk"
      },
      "source": [
        "The following loop brings everthing together, downloading each link, scraping the body text, scrubbing with regex and combining into one long string (this will take a couple minutes to process all articles)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfC5ucx4WYRk"
      },
      "source": [
        "collect = ''\n",
        "for link in new_list:\n",
        "    \n",
        "#make whole url from apnews + list element\n",
        "    url = (\"https://apnews.com\" + link) \n",
        "    print(url)\n",
        "    \n",
        "#get html\n",
        "    s = requests.get(url) \n",
        "    soup = BeautifulSoup(s.content)\n",
        "\n",
        "#grab the article\n",
        "    text = soup.find_all(\"div\", {\"class\": \"Article\"})\n",
        "    text_str = str(text)\n",
        "\n",
        "#regex    \n",
        "    text_str = re.sub(r'<[^>]+>', ' ', text_str)\n",
        "    #text_str = text_str.replace('\\n', ' ')\n",
        "    \n",
        "#collect all texts\n",
        "    collect = collect + text_str\n",
        "print(collect)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa1nTG8MWYRl"
      },
      "source": [
        "Let's check the word count of that long string of combined articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcRnTAe6WYRl"
      },
      "source": [
        "print(str(len(collect.split())) + \" appx words\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbrrCalIWYRl"
      },
      "source": [
        "And finally, we can write the complete collected text to a singe .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AsbmtHhWYRl"
      },
      "source": [
        "text_file = open(\"ap_politics.txt\", \"w\")\n",
        "n = text_file.write(collect)\n",
        "text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}