{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c62992f9",
      "metadata": {
        "id": "c62992f9"
      },
      "source": [
        "## Simple text generation on GPU with AITEXTGEN\n",
        "\n",
        "Here we can test text-generation from various models trained or fine-tuned with *AITEXTGEN*\n",
        "More examples, documentation, and notebooks can be found in the [AITEXTGEN repo](https://github.com/minimaxir/aitextgen/).\n",
        "\n",
        "This demonstrates text-generation *after* training on one of the following two colab notebooks:\n",
        "\n",
        "**Note: as of 1/2023 the two notebooks linked below may be broken, due to an update to aitextgen. And you may need to force an older version of pytorch-lightning with `!pip install pytorch-lightning==1.7.7`\n",
        "\n",
        "[Finetune OpenAI's 124M GPT-2 model (or GPT Neo) on your own dataset (GPU)](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing)\n",
        "\n",
        "[Train a GPT-2 model + tokenizer from scratch (GPU)](https://colab.research.google.com/drive/144MdX5aLqrQ3-YW-po81CQMrD6kpgpYh?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup the Colab environment\n",
        "Before executing any of the cells below, go to **Runtime > Change runtime type**, and make sure **Hardware accelerator** is set to **GPU**\n",
        "<br>\n",
        "Then, install *AITEXTGEN* and its dependencies"
      ],
      "metadata": {
        "id": "e1-oQSdEUX-_"
      },
      "id": "e1-oQSdEUX-_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9e2652",
      "metadata": {
        "id": "9e9e2652"
      },
      "outputs": [],
      "source": [
        "!pip install aitextgen\n",
        "!pip install pytorch-lightning==1.7.7\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "from aitextgen import aitextgen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download a fine-tuned model\n",
        "These are a number of fine-tuned GPT-2 models used for Tivon Rice's projects:\n",
        "\n",
        "*Reading Interiors* https://readinginteriors.hetnieuweinstituut.nl/\n",
        "\n",
        "\n",
        "*   THE CHILD - trained on Roald Dahl's complete works\n",
        "*   THE HOME - trained on Georges Perec's \"Life: A User's Manual\"\n",
        "*   THE OTHER - trained on non-human biology/philosophy: Donna Haraway, Jokob von Uexkull\n",
        "*   THE SPACE - trained on spatial anthropology/philosophy: Marc Auge, Michel deCerteau, Michel Foucault\n",
        "*   THE WORKER - trained on Karl Marx's Kapital 1,2,3\n",
        "\n",
        "*Models for Environmental Literacy* https://www.tivonrice.com/models.html\n",
        "\n",
        "*   ECO FICTION - trained on science fiction from Ursula LeGuin, Octavia Butler, JG Ballard, and others\n",
        "*   ECO PHILOSOPHY - trained on the philosophies of Timothy Morton, Donna Haraway, Paul Virilio, and others\n",
        "*   ECO SCIENCE - trained on the International Panel on Climate Change: Special Report 1.5 - 2008"
      ],
      "metadata": {
        "id": "TsQcD1ROWUfK"
      },
      "id": "TsQcD1ROWUfK"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a model and load\n",
        "#@markdown It may take a minute to downnload and uncompress the model. Afterward, the model will appear as a new folder in the **Files** tab on the left side of Colab.\n",
        "#@markdown <br> If Gooogle Drive is overloaded with requests, this notebook may crash. In this clase, please restart and choose one of the \"mirror\" models.\n",
        "curModel = \"THE CHILD\" #@param [\"THE CHILD\", \"THE HOME\", \"THE OTHER\", \"THE SPACE\", \"THE WORKER\", \"ECO FICTION\", \"ECO PHILOSOPHY\", \"ECO SCIENCE\", \"THE CHILD (mirror)\", \"THE HOME (mirror)\", \"THE OTHER (mirror)\", \"THE SPACE (mirror)\", \"THE WORKER (mirror)\", \"ECO FICTION (mirror)\", \"ECO PHILOSOPHY (mirror)\", \"ECO SCIENCE (mirror)\"]\n",
        "\n",
        "if curModel == \"THE CHILD\":\n",
        "  !gdown --id 118mFUEGujBSIsMo24qKNGe5XUVuucILG\n",
        "elif curModel == \"THE HOME\":\n",
        "  !gdown --id 1g51_Kky_D8F2EcBeIuDUOAWSZcyI6clz\n",
        "elif curModel == \"THE OTHER\":\n",
        "  !gdown --id 1InRcrV7-uaHhksEihV5ysi7noGc0imXz\n",
        "elif curModel == \"THE SPACE\":\n",
        "  !gdown --id 12-NSUMUdNuSW5AFkZRceZJVHZNzsihOV\n",
        "elif curModel == \"THE WORKER\":\n",
        "  !gdown --id 1SGOTrNCMqtj5ABVzWxjUE9K1f4az-5OU\n",
        "elif curModel == \"ECO FICTION\":\n",
        "  !gdown --id 1PYD2XZ4hMEg5irVx_NAk4BnpcxyNQQ19\n",
        "elif curModel == \"ECO PHILOSOPHY\":\n",
        "  !gdown --id 1paYqejqTgZjZezwUtykIAVLPBGZ4vZMI\n",
        "elif curModel == \"ECO SCIENCE\":\n",
        "  !gdown --id 1wlMi2zdmTpwyzDjYHCoJpbhgbOADxVF8\n",
        "elif curModel == \"THE CHILD (mirror)\":\n",
        "  !gdown --id 1naSSYRKpwOZVQpadf15L3MwGgDb_QR_N\n",
        "elif curModel == \"THE HOME (mirror)\":\n",
        "  !gdown --id 1H0LCa4BaAI0ZZjUhmPpew_kokXk9qgw6\n",
        "elif curModel == \"THE OTHER (mirror)\":\n",
        "  !gdown --id 11O3C-VcUrRFKxGeMO3TksbxKNkKU_EDn\n",
        "elif curModel == \"THE SPACE (mirror)\":\n",
        "  !gdown --id 1P9eWJVp8QGO-UDWwU1Sd1M8Ck6rdgdX2\n",
        "elif curModel == \"THE WORKER (mirror)\":\n",
        "  !gdown --id 1icsREXEEbW_q3G9ZPxsODyrLnw1Q6AE-\n",
        "elif curModel == \"ECO FICTION (mirror)\":\n",
        "  !gdown --id 1IZIHS8zfC19q6yNPPOcim8RjA3Fo2D31\n",
        "elif curModel == \"ECO PHILOSOPHY (mirror)\":\n",
        "  !gdown --id 10QLvuzT5a1LOunOI-mUC7I5qXgZEbuTw\n",
        "elif curModel == \"ECO SCIENCE (mirror)\":\n",
        "  !gdown --id 1QQkW4HUdO9Tfn6imm_F2tqqcn0JLJYcq\n",
        "\n",
        "# unpack\n",
        "\n",
        "if curModel == \"THE CHILD\":\n",
        "  !tar -xvf child_py.tar\n",
        "elif curModel == \"THE HOME\":\n",
        "  !tar -xvf home_py.tar\n",
        "elif curModel == \"THE OTHER\":\n",
        "  !tar -xvf other_py.tar\n",
        "elif curModel == \"THE SPACE\":\n",
        "  !tar -xvf space_py.tar\n",
        "elif curModel == \"THE WORKER\":\n",
        "  !tar -xvf worker_py.tar\n",
        "elif curModel == \"ECO FICTION\":\n",
        "  !tar -xvf sf_py.tar\n",
        "elif curModel == \"ECO PHILOSOPHY\":\n",
        "  !tar -xvf phil_py.tar\n",
        "elif curModel == \"ECO SCIENCE\":\n",
        "  !tar -xvf sci_py.tar\n",
        "elif curModel == \"THE CHILD (mirror)\":\n",
        "  !tar -xvf child_py.tar\n",
        "elif curModel == \"THE HOME (mirror)\":\n",
        "  !tar -xvf home_py.tar\n",
        "elif curModel == \"THE OTHER (mirror)\":\n",
        "  !tar -xvf other_py.tar\n",
        "elif curModel == \"THE SPACE (mirror)\":\n",
        "  !tar -xvf space_py.tar\n",
        "elif curModel == \"THE WORKER (mirror)\":\n",
        "  !tar -xvf worker_py.tar\n",
        "elif curModel == \"ECO FICTION (mirror)\":\n",
        "  !tar -xvf sf_py.tar\n",
        "elif curModel == \"ECO PHILOSOPHY (mirror)\":\n",
        "  !tar -xvf phil_py.tar\n",
        "elif curModel == \"ECO SCIENCE (mirror)\":\n",
        "  !tar -xvf sci_py.tar\n"
      ],
      "metadata": {
        "id": "L-YJqHjNXiuK",
        "cellView": "form"
      },
      "id": "L-YJqHjNXiuK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f9c7806b",
      "metadata": {
        "id": "f9c7806b"
      },
      "source": [
        "All of the above models were fine-tuned from the GPT-2 pretrained model. Thus, they use the default GPT-2 tokenizer, and you will only need to define the `model_folder`.\n",
        "<br>\n",
        "Please change the string `\"trained_model\"` to the name of your chosen model, such as `\"child\"`. You can refer to the folders that appear in the Files window on the left (after downoading)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48101c5a",
      "metadata": {
        "id": "48101c5a"
      },
      "outputs": [],
      "source": [
        "ai = aitextgen(model_folder=\"folder\",\n",
        "               to_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de52e4d",
      "metadata": {
        "id": "0de52e4d"
      },
      "source": [
        "Generate conditional text with length, prompt, temp, etc.\n",
        "<br>\n",
        "Generation parameters are further documented here: https://docs.aitextgen.io/generate/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef80e444",
      "metadata": {
        "id": "ef80e444"
      },
      "outputs": [],
      "source": [
        "ai.generate(max_length=250,\n",
        "            prompt=\"In the forest,\",\n",
        "            temperature=0.7,\n",
        "            top_p=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since generation with GPU is very fast, It may be useful to generate multiple outputs at once, adding the `n` parameter (number of outputs)."
      ],
      "metadata": {
        "id": "fDHrGkiGfjrt"
      },
      "id": "fDHrGkiGfjrt"
    },
    {
      "cell_type": "code",
      "source": [
        "ai.generate(n=5,\n",
        "            max_length=250,\n",
        "            prompt=\"In the forest,\",\n",
        "            temperature=0.7,\n",
        "            top_p=0.9)"
      ],
      "metadata": {
        "id": "vf52_rK-e6hH"
      },
      "id": "vf52_rK-e6hH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}