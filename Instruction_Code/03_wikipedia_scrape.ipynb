{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1VITz4mGqjz"
      },
      "source": [
        "### Scraping Text from Wikipedia\n",
        "The *[wikipedia](https://pypi.org/project/wikipedia/)* python library can be used to search and get data from... wikipedia.\n",
        "\n",
        "In addition to going step-by-step through scraping text from Wikipedia, we will cover these core Python topics:\n",
        "<br>\n",
        "Regular Expressions\n",
        "<br>\n",
        "String Operations\n",
        "<br>\n",
        "List Operations\n",
        "<br>\n",
        "Exceptions\n",
        "<br>\n",
        "For loops\n",
        "<br>\n",
        "Try/Except loops\n",
        "<br>\n",
        "Output data to file (.txt)\n",
        "<br>\n",
        "Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOv_yIDtGqj4"
      },
      "source": [
        "pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbEUs78MGqj6"
      },
      "source": [
        "Let's import and use the wikipedia package to simply look at the main text from the DXARTS wikipedia page.\n",
        "Pay attention to the `wikipedia.page( )` and `.content( )` modules.\n",
        "The full list of wikipedia modules can be found [here](https://wikipedia.readthedocs.io/en/latest/code.html#module-wikipedia), and they can return other types of results, such as image urls, internal wikipedia links, external links, summary etc. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-uTQ0XiGqj6"
      },
      "source": [
        "# Import package\n",
        "import wikipedia\n",
        "# Specify the title of the Wikipedia page\n",
        "wiki = wikipedia.page('dxarts')\n",
        "# Extract the plain text content of the page\n",
        "text = wiki.content\n",
        "# Return the text\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWU7b2NnGqj7"
      },
      "source": [
        "Notice all of the formatting characters for headings == and new lines \\n are included. `print( )` will hide these characters and display the formatted text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffY7GoCtGqj8"
      },
      "source": [
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByaxAId7Gqj8"
      },
      "source": [
        "However, we probably don't want this formatting in our text dataset (when using it for training a language model). If we only want to keep the body of each paragraph and nothing else, we will have to do some *scrubbing* with regular expressions:\n",
        "\n",
        "Drop headers surrounded by ‘==’: \n",
        "\n",
        "`re.sub(r'==.*==+', '', text)`\n",
        "    \n",
        "`.` = any character\n",
        "\n",
        "`*` = multiple times\n",
        "\n",
        "`+` = multiple occurrences\n",
        "\n",
        "Replace ‘\\n’ (a new line) with ‘’ (an empty string):\n",
        "\n",
        "`.replace('\\n', ' ')`\n",
        "\n",
        "The output text is a string (you can check this using type(text)) which allows us to use string methods or operations, such as .replace( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VORnH5UbGqj9"
      },
      "source": [
        "# Import regular expression package\n",
        "import re\n",
        "# Clean text as described above\n",
        "text = re.sub(r'==.*==+', '', text)\n",
        "text = text.replace('\\n', ' ')\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeioHIB_Gqj-"
      },
      "source": [
        "### Searching wikipedia\n",
        "The `wikipedia.search( )` method will give the top 10 relevant pages, returned as a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLuVTzL4Gqj_"
      },
      "source": [
        "wiki_search = wikipedia.search(\"art\")\n",
        "print(wiki_search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa5oWJtkGqj_"
      },
      "source": [
        "10 is the default, but can be modified with the argument, `results=100` or any other number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrfkBnweGqkA"
      },
      "source": [
        "wiki_search = wikipedia.search(\"art\", results=100)\n",
        "print(wiki_search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPxB2V6lGqkA"
      },
      "source": [
        "Most of these results seem relevant to \"Art\", but you may want to remove one or more list elements. For example, the singer \"Art Garfunkel,\" or the movie \"O Brother, Where Art Thou?\" So we can remove individual elements while retaining the rest of the list.\n",
        "Note that `remove()` takes exactly one argument, so multiple removes may be needed.\n",
        "\n",
        "This will return an error if any of these named elements are NOT in the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmOEWfgNGqkB"
      },
      "source": [
        "wiki_search.remove('Art Garfunkel')\n",
        "wiki_search.remove('O Brother, Where Art Thou?')\n",
        "# the len() operation will return the length of the list (number of indices) \n",
        "length = len(wiki_search)\n",
        "print(length, wiki_search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phmOVdldGqkB"
      },
      "source": [
        "### Warning:\n",
        "Some of these results may redirect you to a different wiki, which will raise an exception in python. \n",
        "\n",
        "Essentially this occurs when there are multiple wiki pages under the larger \"Art\" umbrella. This *Disambiguation Error* is raised when the page may refer to one of many other pages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGUeeV-kGqkC"
      },
      "source": [
        "wiki = wikipedia.page(title='Art')\n",
        "text = wiki.content\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EX4dNRLGqkC"
      },
      "source": [
        "*Sometimes* this can be avoided by adding the following argument to the `wikipedia.page( )` method:\n",
        "\n",
        "`auto_suggest=False`\n",
        "\n",
        "This asks Wikipedia *not* to suggest other relevant wiki pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C416vqf6GqkC"
      },
      "source": [
        "wiki = wikipedia.page(title='Art', auto_suggest=False)\n",
        "text = wiki.content\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQdQkRYjGqkD"
      },
      "source": [
        "However, this does't always work. To really avoid exceptions/errors, we need to create a try/except loop to ingnore any failed redirects. So first, let's create a list of 5 results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxzJ_oluGqkE"
      },
      "source": [
        "wiki_search = wikipedia.search(\"art\", results=5)\n",
        "print(wiki_search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgAA-cy5GqkE"
      },
      "source": [
        "Next, we use `enumerate()`, `try` and `except` to create a loop that ignores any result that causes an exception. Pay attention to indentation, which indicates which loop/process we are operating in.\n",
        "<br>\n",
        "\n",
        "`collect=''` creates an empty list to save all of our text in.\n",
        "\n",
        "In the next line: `for i, val in enumerate(wiki_search):`... `i` is the index of the current loop, and `val` is the content from our list `(wiki_search)` at that particular index. So on the first loop, `i = 0` and `val = \"Art\"`, on the final loop, `i = 4` and `val = \"Elements of art\"`.\n",
        "<br>\n",
        "\n",
        "Normally this *for* loop could do everything we want, but we also need a process to look for and ignore exceptions, so we use a try/except loop within the for loop. The `try` loop contains everything we want to accomplish: grab the wiki page, extract the text content, add it to our long string of text, and print the results. The `except` loop simply looks for a specific exception from wikipedia, and it it occurs, we pass over it and try the next loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz2MMEeDGqkF"
      },
      "source": [
        "# make an empty string to collect all of the text\n",
        "collect = ''\n",
        "# enumerate() automatically loops through each element in the list \"wiki-search\"\n",
        "for i, val in enumerate(wiki_search):\n",
        "    try:\n",
        "        wiki = wikipedia.page(title=val, auto_suggest=False)\n",
        "# Extract the plain text content of the page\n",
        "        text = wiki.content\n",
        "        collect = collect + text\n",
        "# At the end of each loop, print the index, the result name, and the text    \n",
        "        print(i, val, text)\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        pass\n",
        "        \n",
        "# the except looks for and passes over any errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCPPtoPYGqkF"
      },
      "source": [
        "If we look closely at the resulting text, we'll see that it it was able to scrape indices 0, 3, 4, while passing over 1 & 2.\n",
        "\n",
        "Finally, we could either copy and paste this output, or save it to a .txt file directly from Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0iuU1fhGqkG"
      },
      "source": [
        "text_file = open(\"sample.txt\", \"w\")\n",
        "n = text_file.write(collect)\n",
        "text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgTsOwQIGqkG"
      },
      "source": [
        "## Wrapping everything in to a function\n",
        "\n",
        "If you want to run a large number of searches, or easily switch between different search terms, it can be useful to wrap all of these processes into a function. We have already seen most everything that will go into this function, but now it is defined as its own, complete process.\n",
        "<br>\n",
        "To create a function, use `def`, create a unique name like `wiki_scrape`, then define the kind of arguments you may wany to change on subsequent runs of the function. Here we want to be able to change the wikipedia `search` term, the max number of results `num_results`, and a unique `filename` for the resulting text file. These arguments will be replaced with real names/numbers when we actually run the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMGFP0ekGqkG"
      },
      "source": [
        "def wiki_scrape(search, num_results, filename):\n",
        "    import wikipedia, re\n",
        "#search for n pages    \n",
        "    w_search = wikipedia.search(search, results=num_results)\n",
        "    print(w_search)\n",
        "#scrape all pages and collect all text in a single string\n",
        "    collect = ''\n",
        "    for i, val in enumerate(w_search):\n",
        "#this try/except loop ignores errors from wikipedia        \n",
        "        try:\n",
        "            wiki = wikipedia.page(title=val, auto_suggest=False)\n",
        "            text = wiki.content\n",
        "            collect = collect + text\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "#report which wikis caused errors            \n",
        "            print(\"skipped redirect: \" + val)\n",
        "            pass\n",
        "#regex to scrub formatting        \n",
        "    scrub = collect\n",
        "    scrub = re.sub(r'==.*==+', '', scrub)\n",
        "    scrub = scrub.replace('\\n\\n+', '\\n')\n",
        "#return char and word count\n",
        "    print(str(len(scrub)) + \" characters (w/spaces)\")\n",
        "    print(str(len(scrub.split())) + \" appx words\") \n",
        "    print(collect)\n",
        "#write all text to file\n",
        "    text_file = open(filename, \"w\")\n",
        "    n = text_file.write(scrub)\n",
        "    text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QychsgoCGqkG"
      },
      "source": [
        "Once the function is defined, all we need to do is call the funcion with the three arguments `(search, num_results, filename)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIvkAVZoGqkH"
      },
      "source": [
        "wiki_scrape(\"art\",25,\"art_25.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7AixY8gGqkH"
      },
      "source": [
        "## Function with manual filtering\n",
        "Here, the steps for searching and filtering are broken out of the function, so you can manually remove unwanted entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACbvhZFrGqkH"
      },
      "source": [
        "art_search = wikipedia.search(\"art\", results=50)\n",
        "print(art_search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpDKkstXGqkH"
      },
      "source": [
        "art_search.remove('O Brother, Where Art Thou?')\n",
        "art_search.remove('Art Garfunkel')\n",
        "art_search.remove('Nicholas Art')\n",
        "art_search.remove('Art the Clown')\n",
        "art_search.remove('Art Malik')\n",
        "length = len(art_search)\n",
        "print(length, art_search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTCkzZJFGqkH"
      },
      "source": [
        "def filtered_wiki_scrape(listname, filename):\n",
        "    import wikipedia, re\n",
        "#search for n pages - removed    \n",
        "#   w_search=wikipedia.search(search, results=num_results)\n",
        "    print(listname)\n",
        "#scrape all pages and collect all text in a single string\n",
        "    collect = ''\n",
        "    for i, val in enumerate(listname):\n",
        "#this try/except loop ignores errors from wikipedia        \n",
        "        try:\n",
        "            wiki = wikipedia.page(title=val, auto_suggest=False)\n",
        "            text = wiki.content\n",
        "            collect = collect + text\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "#report which wikis caused errors            \n",
        "            print(\"skipped redirect: \" + val)\n",
        "            pass\n",
        "# regex to scrub formatting        \n",
        "    scrub = collect\n",
        "    scrub = re.sub(r'==.*==+', '', scrub)\n",
        "    #scrub = scrub.replace('\\n', ' ')\n",
        "# return char and word count\n",
        "    print(str(len(scrub)) + \" characters (w/spaces)\")\n",
        "    print(str(len(scrub.split())) + \" appx words\") \n",
        "    print(collect)\n",
        "# write all text to file\n",
        "    text_file = open(filename, \"w\")\n",
        "    n = text_file.write(scrub)\n",
        "    text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXSD2AJmGqkI"
      },
      "source": [
        "filtered_wiki_scrape(art_search, 'filtered.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q48KRWD3GqkI"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}